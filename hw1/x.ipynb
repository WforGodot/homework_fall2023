{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  C:\\Users\\proje\\Documents\\GitHub\\homework_fall2023\\hw1\\cs285\\scripts\\../../data\\q1_bc_hopper_Hopper-v4_23-06-2024_21-31-33\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... cs285/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1012.3624877929688\n",
      "Eval_StdReturn : 385.5955505371094\n",
      "Eval_MaxReturn : 1522.047119140625\n",
      "Eval_MinReturn : 589.615234375\n",
      "Eval_AverageEpLen : 347.3333333333333\n",
      "Train_AverageReturn : 3717.5129936182307\n",
      "Train_StdReturn : 0.3530361779417035\n",
      "Train_MaxReturn : 3717.8660297961724\n",
      "Train_MinReturn : 3717.159957440289\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : -1.100154995918274\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 4.458888292312622\n",
      "Initial_DataCollection_AverageReturn : 3717.5129936182307\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\proje\\anaconda3\\envs\\cs285\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\proje\\anaconda3\\envs\\cs285\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "!python cs285/scripts/run_hw1.py \\\n",
    "--expert_policy_file cs285/policies/experts/Hopper.pkl \\\n",
    "--env_name Hopper-v4 --exp_name bc_hopper --n_iter 1 \\\n",
    "--expert_data cs285/expert_data/expert_data_Hopper-v4.pkl \\\n",
    "--video_log_freq -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "c:\\Users\\proje\\anaconda3\\envs\\cs285\\lib\\site-packages\\numpy\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import numpy;print(numpy.__version__);print(numpy.__file__)\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  C:\\Users\\proje\\Documents\\GitHub\\homework_fall2023\\hw1\\cs285\\scripts\\../../data\\q2_dagger_hopper_Hopper-v4_23-06-2024_21-54-14\n",
      "########################\n",
      "GPU not detected. Defaulting to CPU.\n",
      "Loading expert policy from... cs285/policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1012.3624877929688\n",
      "Eval_StdReturn : 385.5955505371094\n",
      "Eval_MaxReturn : 1522.047119140625\n",
      "Eval_MinReturn : 589.615234375\n",
      "Eval_AverageEpLen : 347.3333333333333\n",
      "Train_AverageReturn : 3717.5129936182307\n",
      "Train_StdReturn : 0.3530361779417035\n",
      "Train_MaxReturn : 3717.8660297961724\n",
      "Train_MinReturn : 3717.159957440289\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : -1.100154995918274\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 3.651218891143799\n",
      "Initial_DataCollection_AverageReturn : 3717.5129936182307\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 2413.3349609375\n",
      "Eval_StdReturn : 1198.6009521484375\n",
      "Eval_MaxReturn : 3611.935791015625\n",
      "Eval_MinReturn : 1214.73388671875\n",
      "Eval_AverageEpLen : 674.5\n",
      "Train_AverageReturn : 794.9976196289062\n",
      "Train_StdReturn : 216.2172088623047\n",
      "Train_MaxReturn : 1071.664306640625\n",
      "Train_MinReturn : 537.071044921875\n",
      "Train_AverageEpLen : 273.5\n",
      "Training Loss : -1.008007287979126\n",
      "Train_EnvstepsSoFar : 1094\n",
      "TimeSinceStart : 8.09822964668274\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1337.7745361328125\n",
      "Eval_StdReturn : 213.71578979492188\n",
      "Eval_MaxReturn : 1639.9097900390625\n",
      "Eval_MinReturn : 1179.826416015625\n",
      "Eval_AverageEpLen : 374.0\n",
      "Train_AverageReturn : 3606.60009765625\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3606.60009765625\n",
      "Train_MinReturn : 3606.60009765625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : -1.134356141090393\n",
      "Train_EnvstepsSoFar : 2094\n",
      "TimeSinceStart : 11.496869087219238\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 2338.96826171875\n",
      "Eval_StdReturn : 317.7127685546875\n",
      "Eval_MaxReturn : 2656.68115234375\n",
      "Eval_MinReturn : 2021.255615234375\n",
      "Eval_AverageEpLen : 622.0\n",
      "Train_AverageReturn : 1928.471923828125\n",
      "Train_StdReturn : 556.985595703125\n",
      "Train_MaxReturn : 2485.45751953125\n",
      "Train_MinReturn : 1371.486328125\n",
      "Train_AverageEpLen : 522.5\n",
      "Training Loss : -1.2810622453689575\n",
      "Train_EnvstepsSoFar : 3139\n",
      "TimeSinceStart : 15.926952123641968\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1555.2689208984375\n",
      "Eval_StdReturn : 240.4750518798828\n",
      "Eval_MaxReturn : 1881.924560546875\n",
      "Eval_MinReturn : 1309.9996337890625\n",
      "Eval_AverageEpLen : 427.3333333333333\n",
      "Train_AverageReturn : 3106.780029296875\n",
      "Train_StdReturn : 515.173095703125\n",
      "Train_MaxReturn : 3621.953125\n",
      "Train_MinReturn : 2591.60693359375\n",
      "Train_AverageEpLen : 822.5\n",
      "Training Loss : -1.2506370544433594\n",
      "Train_EnvstepsSoFar : 4784\n",
      "TimeSinceStart : 19.796834230422974\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3703.7353515625\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3703.7353515625\n",
      "Eval_MinReturn : 3703.7353515625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1365.53759765625\n",
      "Train_StdReturn : 155.00498962402344\n",
      "Train_MaxReturn : 1548.77685546875\n",
      "Train_MinReturn : 1169.7193603515625\n",
      "Train_AverageEpLen : 380.3333333333333\n",
      "Training Loss : -1.2377835512161255\n",
      "Train_EnvstepsSoFar : 5925\n",
      "TimeSinceStart : 23.626026153564453\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3708.862060546875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3708.862060546875\n",
      "Eval_MinReturn : 3708.862060546875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3708.87109375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3708.87109375\n",
      "Train_MinReturn : 3708.87109375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : -1.1375443935394287\n",
      "Train_EnvstepsSoFar : 6925\n",
      "TimeSinceStart : 27.013476371765137\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3713.44921875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3713.44921875\n",
      "Eval_MinReturn : 3713.44921875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3709.36669921875\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3709.36669921875\n",
      "Train_MinReturn : 3709.36669921875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : -1.2625985145568848\n",
      "Train_EnvstepsSoFar : 7925\n",
      "TimeSinceStart : 30.481605291366577\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3711.998046875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3711.998046875\n",
      "Eval_MinReturn : 3711.998046875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3715.0927734375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3715.0927734375\n",
      "Train_MinReturn : 3715.0927734375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : -1.4610962867736816\n",
      "Train_EnvstepsSoFar : 8925\n",
      "TimeSinceStart : 33.954882860183716\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3717.6123046875\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 3717.6123046875\n",
      "Eval_MinReturn : 3717.6123046875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3709.7744140625\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3709.7744140625\n",
      "Train_MinReturn : 3709.7744140625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Training Loss : -1.4423140287399292\n",
      "Train_EnvstepsSoFar : 9925\n",
      "TimeSinceStart : 37.4865882396698\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\proje\\anaconda3\\envs\\cs285\\lib\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\proje\\anaconda3\\envs\\cs285\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "!python cs285/scripts/run_hw1.py \\\n",
    "--expert_policy_file cs285/policies/experts/Hopper.pkl \\\n",
    "--env_name Hopper-v4 --exp_name dagger_hopper --n_iter 10 \\\n",
    "--do_dagger --expert_data cs285/expert_data/expert_data_Hopper-v4.pkl \\\n",
    "--video_log_freq -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs285",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
